{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECG Classification using PyStark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#these characters are used as part of the compression algorithm\n",
    "\n",
    "gsm = list(r'@£$¥èéùìòÇØøÅå_^{}[~]|€ӔӕßÉ!#¤%&()*+,-./:;<?¡§¿äöñüàÀÁÂÃÄÈÊËÌÍÎÏÐÑÒÓÔÕÖÙÚÛÜÝÞþáâãçêëíîïðóôõúûýabcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "gsm.append(\"\\\\\")\n",
    "range_list = [0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000]\n",
    "for i in range_list:\n",
    "    gsm.append(\"Num\"+str(i))\n",
    "\n",
    "gsm.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the encode function compresses an ECG segment(in numerical format) to an alphanumeric value\n",
    "from queue import Queue\n",
    "import math\n",
    "def encode(lines):\n",
    "    \n",
    "    q = Queue()\n",
    "    for j in map(int,lines.split(',')):\n",
    "        q.put(j)\n",
    "    \n",
    "    print(q)\n",
    "    \n",
    "    noOfSmpl = 1\n",
    "    intDiff = []\n",
    "    endOfEcg = False\n",
    "    rrOn = True\n",
    "    rrCount = 0\n",
    "    ecgSmpl1 = q.get()\n",
    "    rrComp1 = 0\n",
    "    encSB = []\n",
    "    rrSB = []\n",
    "    gsm = list(\n",
    "        r'@£$¥èéùìòÇØøÅå_^{}[~]|€ӔӕßÉ!#¤%&()*+,-./:;<?¡§¿äöñüàÀÁÂÃÄÈÊËÌÍÎÏÐÑÒÓÔÕÖÙÚÛÜÝÞþáâãçêëíîïðóôõúûýabcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "    gsm.append(\"\\\\\")\n",
    "\n",
    "\n",
    "    intHrv = 0\n",
    "    while endOfEcg != True and not q.empty():\n",
    "\n",
    "        intDiff = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        for i in range(0, 8):\n",
    "            ecgSmpl2 = q.get()\n",
    "            noOfSmpl = noOfSmpl + 1\n",
    "            diff = ecgSmpl2 - ecgSmpl1\n",
    "\n",
    "            try:\n",
    "                if (math.ceil(diff) - diff) > 0.5:\n",
    "                    intDiff[i] = int(math.floor(diff))  # there was an (int) here\n",
    "                else:\n",
    "                    intDiff[i] = int(math.ceil(diff))\n",
    "            except:\n",
    "                print(\"ERROR\", intDiff[i], math.floor(diff))\n",
    "\n",
    "            ecgSmpl1 = ecgSmpl2\n",
    "\n",
    "        if q.empty():\n",
    "            endOfEcg = True\n",
    "\n",
    "        # intDiff is a list of 8 elements\n",
    "        signVal = 0\n",
    "\n",
    "        for i in range(0, 8):\n",
    "            if intDiff[i] <= -1:\n",
    "                if i == 0:\n",
    "                    signVal = signVal + 1\n",
    "                elif i == 1:\n",
    "                    signVal = signVal + 2\n",
    "                elif i == 2:\n",
    "                    signVal = signVal + 4\n",
    "                elif i == 3:\n",
    "                    signVal = signVal + 8\n",
    "                elif i == 4:\n",
    "                    signVal = signVal + 16\n",
    "                elif i == 5:\n",
    "                    signVal = signVal + 32\n",
    "                elif i == 6:\n",
    "                    signVal = signVal + 64\n",
    "                elif i == 7:\n",
    "                    encSB.append('')\n",
    "\n",
    "                intDiff[i] = abs(intDiff[i])\n",
    "\n",
    "        encSB.append(gsm[signVal])\n",
    "\n",
    "\n",
    "        # for q = 7\n",
    "        for i in range(0, 7, 2):\n",
    "            # compresses two single digits as one\n",
    "            if (intDiff[i] < 10) and (intDiff[i + 1] < 10):\n",
    "                join = intDiff[i] * 10 + intDiff[i + 1]\n",
    "                encSB.append(gsm[join])\n",
    "            # compresses all values less than 147\n",
    "            elif (intDiff[i] < 47) and (intDiff[i + 1] < 47):\n",
    "                encSB.append(gsm[intDiff[i] + 100])\n",
    "                encSB.append(gsm[intDiff[i + 1] + 100])\n",
    "            else:\n",
    "                encSB.append(str(intDiff[i]))\n",
    "                encSB.append('\"')\n",
    "                encSB.append(str(intDiff[i + 1]))\n",
    "                encSB.append('\"')\n",
    "                # rr detection proceeding\n",
    "                if rrOn == True:\n",
    "                    if (rrComp1 < intDiff[i]) and (intDiff[i] < intDiff[i + 1]):\n",
    "                        rrComp1 = intDiff[i + 1]\n",
    "                    else:\n",
    "                        if rrComp1 < intDiff[i]:\n",
    "                            rrCount = rrCount - 1\n",
    "                        rrVal = float(rrCount)\n",
    "                        rrVal = rrVal / 360\n",
    "                        rrSB.append(rrVal)\n",
    "                        rrCount = 0\n",
    "                        rrOn = False\n",
    "                        intHrv = intHrv + 1\n",
    "\n",
    "    str_encSB = ''.join(encSB)\n",
    "\n",
    "    return str_encSB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function derives the features from a compressed ECG segment\n",
    "\n",
    "def compress(segment):\n",
    "    i = 0\n",
    "    range_list = [0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000]\n",
    "    alpha_count = {}\n",
    "    while i < len(segment):\n",
    "\n",
    "        if segment[i].isdigit():\n",
    "\n",
    "            s = ''\n",
    "            while (segment[i] != '\"'):\n",
    "                s = s + segment[i]\n",
    "                i = i + 1\n",
    "\n",
    "            for g in range(len(range_list)):\n",
    "                if int(s) <= range_list[g]:\n",
    "                    t = \"Num\" + str(g)\n",
    "                    break\n",
    "\n",
    "            if t not in alpha_count.keys():\n",
    "                alpha_count[t] = 1\n",
    "            else:\n",
    "                alpha_count[t] = alpha_count[t] + 1\n",
    "\n",
    "\n",
    "        else:\n",
    "            s = segment[i]\n",
    "            if s not in alpha_count.keys():\n",
    "                alpha_count[s] = 1\n",
    "            else:\n",
    "                alpha_count[s] = alpha_count[s] + 1\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "\n",
    "    output = []\n",
    "    for val in gsm:\n",
    "        if val in alpha_count.keys():\n",
    "            output.append(alpha_count[val])\n",
    "        else:\n",
    "            output.append(0)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The main program starts from here\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defining a spark session\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all the data is loaded in an RDD at once\n",
    "rdd = sc.textFile(r'C:\\Users\\Shweta\\PycharmProjects\\untitled\\new_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is what one line of the data looks like\n",
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now the encode function is mapped to each line of the RDD\n",
    "data1 = rdd.map(encode)\n",
    "data1.take(5)             #we have shown 5 lines of compressed ECG signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now features are derived from each line of the RDD\n",
    "data2 = data1.map(compress)\n",
    "temp = data2.take(5)           #we have shown features derived from 5 ECG segments\n",
    "\n",
    "for i in temp:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the indivisual features we shall be looking for\n",
    "keys = gsm\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we create a DataFrame from our RDD\n",
    "a = sqlContext.createDataFrame(data2, ['x'+str(i) for i in range(0,len(keys))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is the schema of the DataFrame\n",
    "#a.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We take the class of the input features in a list\n",
    "fz = open(r'C:\\Users\\Shweta\\PycharmProjects\\untitled\\new_output_class.txt', 'r')\n",
    "cls = fz.read()\n",
    "fz.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# c = 0\n",
    "# for i in cls.split(','):\n",
    "#     if i != '':\n",
    "#         c += 1\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we create another DataFrame consisting of only class column\n",
    "b = sqlContext.createDataFrame([(int(i),) for i in cls.split(',') if i != ''], ['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we consider the schema of the class column\n",
    "b.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now we join the two dataframes(for features and class) togeather in the final dataFrame\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "a = a.withColumn(\"row_idx\", monotonically_increasing_id())\n",
    "b = b.withColumn(\"row_idx\", monotonically_increasing_id())\n",
    "final_df = b.join(a, b.row_idx == a.row_idx).drop(\"row_idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is what the final dataFrame looks like\n",
    "final_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import StringIndexer\n",
    "# # build indexer\n",
    "# string_indexer = StringIndexer(inputCol='class', outputCol='in_class')\n",
    "\n",
    "# # learn the model\n",
    "# string_indexer_model = string_indexer.fit(final_df)\n",
    "\n",
    "# # transform the data\n",
    "# df_new = string_indexer_model.transform(final_df)\n",
    "\n",
    "# # resulting df\n",
    "# df_new.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We consider the columns of the final DataFrame\n",
    "#final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this part of the code runs the Machine Learning Algorithm\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "\n",
    "#assemble all the festures in an assembler\n",
    "Cols= ['x'+ str(c) for c in range(0, 167)]\n",
    "Assembler = VectorAssembler ( inputCols = Cols, outputCol='features')\n",
    "\n",
    "#apply feature selection using Chisquared feature selection\n",
    "selector = ChiSqSelector(numTopFeatures=13, featuresCol='features',outputCol='selectedFeatures', labelCol='class')\n",
    "\n",
    "gmm = GaussianMixture(featuresCol='selectedFeatures', predictionCol = 'prediction', k= 4, probabilityCol='probability', maxIter= 100, seed=1) \n",
    "pipeline = Pipeline(stages=[Assembler, selector,gmm])\n",
    "\n",
    "(train, test) = final_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = pipeline.fit(train)\n",
    "# result = model.transform(final_df).select('features', 'selectedFeatures', 'probability', 'prediction', 'class')\n",
    "# result.show(3200)\n",
    "# d = result.select('prediction', 'class').toPandas()\n",
    "# d.loc[d['class'] == 1, 'prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the test data is fit into the model\n",
    "model123 = pipeline.fit(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we select just a couple of features from the entire dataset\n",
    "result123 = model123.transform(final_df).select('features', 'selectedFeatures', 'probability','prediction', 'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in here we see how features contains 168 features out of which only 13 features are selected\n",
    "result123.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d123 = result123.select('prediction', 'class').toPandas()\n",
    "d123.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build a class matrix to check the accuracy\n",
    "pospos = 0\n",
    "negneg = 0\n",
    "posneg = 0\n",
    "negpos = 0\n",
    "for iterator in range(len(d123)):\n",
    "    if d123['prediction'][iterator] != d123['class'][iterator]:\n",
    "        if d123['class'][iterator] == 0:\n",
    "            pospos = pospos + 1\n",
    "        if d123['class'][iterator] == 1:\n",
    "            negneg = negneg + 1\n",
    "    else:\n",
    "        if d123['class'][iterator] == 0:\n",
    "            posneg = posneg + 1\n",
    "        if d123['class'][iterator] == 1:\n",
    "            negpos = negpos + 1\n",
    "\n",
    "print(\"\\n\\n\\n\\n\\n\")\n",
    "print(\"Finished making predictions for test data.\\n\")\n",
    "print(\"x-axis: predicted class\")\n",
    "print(\"y-axis: actual class\")\n",
    "print(\"\\n\")\n",
    "print(\"\\tcls pos\\tneg\")\n",
    "print(\"\\tpos\", pospos,posneg)\n",
    "print(\"\\tneg\", negpos,negneg)\n",
    "\n",
    "accuracy = (pospos+negneg)/(pospos+negneg+posneg+negpos)*100\n",
    "print(\"\\nAccuracy of the model : \", accuracy,\"%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
